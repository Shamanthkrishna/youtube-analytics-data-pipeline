{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fde030e-3f1f-4e7e-a586-36c481d9ac0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-api-python-client in /local_disk0/.ephemeral_nfs/envs/pythonEnv-66cee58c-0eda-45c5-bb37-7f92df32cfb8/lib/python3.12/site-packages (2.188.0)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/lib/python3/dist-packages (from google-api-python-client) (0.20.4)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 in /databricks/python3/lib/python3.12/site-packages (from google-api-python-client) (2.40.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-66cee58c-0eda-45c5-bb37-7f92df32cfb8/lib/python3.12/site-packages (from google-api-python-client) (0.3.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in /databricks/python3/lib/python3.12/site-packages (from google-api-python-client) (2.20.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-66cee58c-0eda-45c5-bb37-7f92df32cfb8/lib/python3.12/site-packages (from google-api-python-client) (4.2.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /databricks/python3/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.65.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /databricks/python3/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (5.29.4)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /databricks/python3/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.26.1)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /databricks/python3/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /databricks/python3/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (5.5.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (4.9.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /databricks/python3/lib/python3.12/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client) (3.2.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2025.1.31)\n",
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83aa5f0b-f6d1-4471-9b15-fb46eaaa8241",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:139)\n",
       "\tat scala.Option.getOrElse(Option.scala:201)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:139)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:136)\n",
       "\tat scala.collection.immutable.Range.foreach(Range.scala:192)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:136)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:721)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:441)\n",
       "\tat scala.Option.getOrElse(Option.scala:201)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:441)\n",
       "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:486)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:768)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:512)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:632)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$5(UsageLogging.scala:659)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:80)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:172)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:153)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:80)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:627)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:521)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:80)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:513)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:477)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:80)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:740)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur.handleDriverRequest$1(Chauffeur.scala:943)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur.$anonfun$handleDriverRequests$2(Chauffeur.scala:970)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:632)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$5(UsageLogging.scala:659)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur.withAttributionContext(Chauffeur.scala:167)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:172)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:153)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur.withAttributionTags(Chauffeur.scala:167)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:627)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:521)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur.recordOperationWithResultTags(Chauffeur.scala:167)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:969)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur.handleDriverRequests(Chauffeur.scala:1020)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$com$databricks$spark$chauffeur$Chauffeur$$nestedInanon$$receiveInternal$1.applyOrElse(Chauffeur.scala:828)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$com$databricks$spark$chauffeur$Chauffeur$$nestedInanon$$receiveInternal$1.applyOrElse(Chauffeur.scala:730)\n",
       "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:726)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:721)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:178)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:204)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:204)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:175)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:165)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:512)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:632)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$5(UsageLogging.scala:659)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:172)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:153)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:627)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:521)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:513)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:477)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.executeWithLogging$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:165)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:997)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:917)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:557)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:522)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$12(ActivityContextFactory.scala:1132)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:68)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$2(ActivityContextFactory.scala:1132)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:1094)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:1075)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$41(ActivityContextFactory.scala:437)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:68)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:437)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:522)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:417)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:111)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:46)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:111)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:46)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:93)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Cancelled"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandCancelledException",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:139)",
        "\tat scala.Option.getOrElse(Option.scala:201)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:139)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:136)",
        "\tat scala.collection.immutable.Range.foreach(Range.scala:192)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:136)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:721)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:441)",
        "\tat scala.Option.getOrElse(Option.scala:201)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:441)",
        "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:486)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:768)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:512)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:632)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$5(UsageLogging.scala:659)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:80)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:172)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:153)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:80)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:627)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:521)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:80)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:513)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:477)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:80)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:740)",
        "\tat com.databricks.spark.chauffeur.Chauffeur.handleDriverRequest$1(Chauffeur.scala:943)",
        "\tat com.databricks.spark.chauffeur.Chauffeur.$anonfun$handleDriverRequests$2(Chauffeur.scala:970)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:632)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$5(UsageLogging.scala:659)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)",
        "\tat com.databricks.spark.chauffeur.Chauffeur.withAttributionContext(Chauffeur.scala:167)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:172)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:153)",
        "\tat com.databricks.spark.chauffeur.Chauffeur.withAttributionTags(Chauffeur.scala:167)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:627)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:521)",
        "\tat com.databricks.spark.chauffeur.Chauffeur.recordOperationWithResultTags(Chauffeur.scala:167)",
        "\tat com.databricks.spark.chauffeur.Chauffeur.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:969)",
        "\tat com.databricks.spark.chauffeur.Chauffeur.handleDriverRequests(Chauffeur.scala:1020)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$com$databricks$spark$chauffeur$Chauffeur$$nestedInanon$$receiveInternal$1.applyOrElse(Chauffeur.scala:828)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$com$databricks$spark$chauffeur$Chauffeur$$nestedInanon$$receiveInternal$1.applyOrElse(Chauffeur.scala:730)",
        "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:726)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:721)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:178)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:204)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:204)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:175)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:165)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:512)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:632)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$5(UsageLogging.scala:659)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:172)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:153)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:627)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:521)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:513)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:477)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
        "\tat com.databricks.rpc.ServerBackend.executeWithLogging$1(ServerBackend.scala:147)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:165)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:997)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:917)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:557)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:522)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$12(ActivityContextFactory.scala:1132)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:68)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$2(ActivityContextFactory.scala:1132)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:1094)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:1075)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$41(ActivityContextFactory.scala:437)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:68)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:437)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:522)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:417)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:111)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:117)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:115)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:112)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:46)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:111)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:46)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:93)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.base/java.lang.Thread.run(Thread.java:840)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== CREATE JOB WIDGETS =====\n",
    "# Not needed for production\n",
    "# dbutils.widgets.text(\"YOUTUBE_API_KEY\", \"\")\n",
    "# dbutils.widgets.text(\"S3_BUCKET\", \"\")\n",
    "# dbutils.widgets.text(\"CHANNEL_CSV_PATH\", \"\")\n",
    "# dbutils.widgets.text(\"SAVE_FORMAT\", \"csv\")\n",
    "# dbutils.widgets.text(\"UPLOAD_TO_S3\", \"true\")\n",
    "# dbutils.widgets.text(\"AWS_ACCESS_KEY_ID\", \"\")\n",
    "# dbutils.widgets.text(\"AWS_SECRET_ACCESS_KEY\", \"\")\n",
    "\n",
    "# ===== JOB PARAMETERS =====\n",
    "YOUTUBE_API_KEY = dbutils.widgets.get(\"YOUTUBE_API_KEY\")\n",
    "S3_BUCKET = dbutils.widgets.get(\"S3_BUCKET\")\n",
    "CHANNEL_CSV_PATH = dbutils.widgets.get(\"CHANNEL_CSV_PATH\")\n",
    "SAVE_FORMAT = dbutils.widgets.get(\"SAVE_FORMAT\")\n",
    "UPLOAD_TO_S3 = dbutils.widgets.get(\"UPLOAD_TO_S3\").lower() == \"true\"\n",
    "AWS_ACCESS_KEY_ID = dbutils.widgets.get(\"AWS_ACCESS_KEY_ID\")\n",
    "AWS_SECRET_ACCESS_KEY = dbutils.widgets.get(\"AWS_SECRET_ACCESS_KEY\")\n",
    "\n",
    "\n",
    "'''\n",
    "YouTube Analytics Data Ingestion Script - Databricks Version\n",
    "\n",
    "This script extracts comprehensive data from YouTube channels using the YouTube Data API v3.\n",
    "Optimized for Databricks environment with DBFS integration.\n",
    "\n",
    "OPTIMIZATION FEATURES:\n",
    "- Incremental fetching: Only fetches new videos since last run\n",
    "- Video ID caching: Stores previously fetched video IDs to avoid duplicates\n",
    "- Reduced API calls: Skips re-fetching details for already processed videos\n",
    "\n",
    "Data Extracted:\n",
    "Channel-level:\n",
    "    - Channel ID\n",
    "    - Channel name\n",
    "    - Subscribers count\n",
    "    - Total views\n",
    "    - Video count\n",
    "\n",
    "Video-level:\n",
    "    - Video ID\n",
    "    - Title\n",
    "    - Published date\n",
    "    - View count\n",
    "    - Like count\n",
    "    - Comment count\n",
    "    - Duration\n",
    "\n",
    "Optional (bonus):\n",
    "    - Tags\n",
    "    - Category\n",
    "    - Language\n",
    "'''\n",
    "\n",
    "# Import required libraries\n",
    "from googleapiclient.discovery import build  # Google API client for YouTube API\n",
    "import pandas as pd  # Data manipulation and CSV export\n",
    "import isodate  # Parse ISO 8601 duration format (e.g., PT4M13S)\n",
    "import datetime  # Timestamp generation for file naming\n",
    "import logging\n",
    "import json  # For caching metadata\n",
    "\n",
    "\n",
    "# ===== DATABRICKS CONFIGURATION =====\n",
    "# In Databricks, use dbutils for secrets management and file operations\n",
    "# Secrets should be stored in Databricks Secret Scopes\n",
    "# Example: dbutils.secrets.put(scope=\"youtube-api\", key=\"api-key\", value=\"YOUR_API_KEY\")\n",
    "\n",
    "try:\n",
    "    # Try to get dbutils (available in Databricks notebooks)\n",
    "    from pyspark.dbutils import DBUtils\n",
    "    dbutils = DBUtils(spark)\n",
    "    IS_DATABRICKS = True\n",
    "except ImportError:\n",
    "    # Fallback for testing outside Databricks\n",
    "    IS_DATABRICKS = False\n",
    "    print(\"Warning: Not running in Databricks environment. Using fallback configuration.\")\n",
    "\n",
    "\n",
    "# ===== LOGGING CONFIGURATION =====\n",
    "# Set up logging for Databricks environment\n",
    "# Logs will be visible in the notebook output and cluster logs\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# ===== DBFS PATH CONFIGURATION =====\n",
    "# Databricks File System (DBFS) paths for storing data\n",
    "# Use /dbfs/ prefix for local file system access or dbfs:/ for Spark operations\n",
    "DBFS_BASE_PATH = '/Workspace/Users/shamanthkrishna0@gmail.com/youtube-analytics-data-pipeline'\n",
    "DBFS_OUTPUT_PATH = f'{DBFS_BASE_PATH}/Output'\n",
    "DBFS_LOGS_PATH = f'{DBFS_BASE_PATH}/Logs'\n",
    "DBFS_INPUT_PATH = f'{DBFS_BASE_PATH}/input'\n",
    "DBFS_CACHE_PATH = f'{DBFS_BASE_PATH}/cache'  # New: Cache directory for incremental fetching\n",
    "\n",
    "\n",
    "def setup_dbfs_directories():\n",
    "    \"\"\"\n",
    "    Create necessary DBFS directories for the pipeline.\n",
    "    Uses dbutils.fs for Databricks-native file operations.\n",
    "    \"\"\"\n",
    "    if IS_DATABRICKS:\n",
    "        # Create directories using dbutils\n",
    "        for path in [DBFS_OUTPUT_PATH, DBFS_LOGS_PATH, DBFS_INPUT_PATH, DBFS_CACHE_PATH]:\n",
    "            dbfs_path = path.replace('/dbfs', 'dbfs:')\n",
    "            try:\n",
    "                dbutils.fs.mkdirs(dbfs_path)\n",
    "                logger.info(f\"Created/verified directory: {dbfs_path}\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not create directory {dbfs_path}: {e}\")\n",
    "    else:\n",
    "        # Fallback for local testing\n",
    "        import os\n",
    "        for path in [DBFS_OUTPUT_PATH, DBFS_LOGS_PATH, DBFS_INPUT_PATH, DBFS_CACHE_PATH]:\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "# ===== CACHING FUNCTIONS FOR INCREMENTAL FETCHING =====\n",
    "def get_cache_file_path(channel_id):\n",
    "    \"\"\"Get the cache file path for a specific channel.\"\"\"\n",
    "    return f'{DBFS_CACHE_PATH}/channel_{channel_id}_cache.json'\n",
    "\n",
    "\n",
    "def load_channel_cache(channel_id):\n",
    "    \"\"\"\n",
    "    Load cached data for a channel including:\n",
    "    - Last fetch timestamp\n",
    "    - Previously fetched video IDs\n",
    "    \n",
    "    Returns:\n",
    "        dict: Cache data with 'last_fetch_time' and 'video_ids' keys\n",
    "              Returns empty cache if file doesn't exist\n",
    "    \"\"\"\n",
    "    cache_file = get_cache_file_path(channel_id)\n",
    "    try:\n",
    "        with open(cache_file, 'r') as f:\n",
    "            cache_data = json.load(f)\n",
    "            logger.info(f\"Loaded cache for channel {channel_id}: {len(cache_data.get('video_ids', []))} cached videos\")\n",
    "            return cache_data\n",
    "    except FileNotFoundError:\n",
    "        logger.info(f\"No cache found for channel {channel_id}, will fetch all videos\")\n",
    "        return {'last_fetch_time': None, 'video_ids': []}\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error loading cache for {channel_id}: {e}\")\n",
    "        return {'last_fetch_time': None, 'video_ids': []}\n",
    "\n",
    "\n",
    "def save_channel_cache(channel_id, video_ids, fetch_time=None):\n",
    "    \"\"\"\n",
    "    Save cache data for a channel.\n",
    "    \n",
    "    Parameters:\n",
    "        channel_id (str): The channel ID\n",
    "        video_ids (list): List of all video IDs fetched (cumulative)\n",
    "        fetch_time (str): ISO format timestamp of this fetch\n",
    "    \"\"\"\n",
    "    cache_file = get_cache_file_path(channel_id)\n",
    "    if fetch_time is None:\n",
    "        fetch_time = datetime.datetime.utcnow().isoformat() + 'Z'\n",
    "    \n",
    "    cache_data = {\n",
    "        'last_fetch_time': fetch_time,\n",
    "        'video_ids': list(set(video_ids)),  # Deduplicate\n",
    "        'updated_at': datetime.datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with open(cache_file, 'w') as f:\n",
    "            json.dump(cache_data, f, indent=2)\n",
    "        logger.info(f\"Saved cache for channel {channel_id}: {len(cache_data['video_ids'])} total videos\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving cache for {channel_id}: {e}\")\n",
    "\n",
    "\n",
    "def get_api_credentials():\n",
    "    \"\"\"\n",
    "    Retrieve API credentials from Databricks secrets.\n",
    "    \n",
    "    In Databricks, create a secret scope and add your API key:\n",
    "    - Scope name: 'youtube-api' (or customize)\n",
    "    - Key name: 'api-key'\n",
    "    \n",
    "    Command to create secret scope (run in Databricks notebook):\n",
    "    dbutils.secrets.help()\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (API_KEY, S3_BUCKET_NAME)\n",
    "    \"\"\"\n",
    "    if IS_DATABRICKS:\n",
    "        try:\n",
    "            # Retrieve API key from Databricks secrets\n",
    "            # Replace 'youtube-api' with your actual secret scope name\n",
    "            # Optional: Get S3 bucket name from secrets\n",
    "            API_KEY = YOUTUBE_API_KEY\n",
    "            bucket = S3_BUCKET\n",
    "            \n",
    "            logger.info(\"Successfully retrieved credentials from Databricks secrets\")\n",
    "            return API_KEY, bucket\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error retrieving secrets: {e}\")\n",
    "            raise ValueError(\"Failed to retrieve API credentials from Databricks secrets. \"\n",
    "                           \"Please configure the 'youtube-api' secret scope.\")\n",
    "    else:\n",
    "        # Fallback for local testing\n",
    "        import os\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv()\n",
    "        return API_KEY, S3_BUCKET\n",
    "\n",
    "\n",
    "def get_channel_stats(youtube, channel_id):\n",
    "    \"\"\"\n",
    "    Fetch channel-level statistics from YouTube API.\n",
    "    \n",
    "    This function retrieves basic information and statistics for a YouTube channel,\n",
    "    including subscriber count, total views, and number of videos published.\n",
    "    \n",
    "    Parameters:\n",
    "        youtube: The YouTube API client object\n",
    "        channel_id (str): The unique identifier for the YouTube channel\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing channel statistics with keys:\n",
    "              - channel_id: The channel's unique identifier\n",
    "              - channel_name: The display name of the channel\n",
    "              - subscribers: Total subscriber count\n",
    "              - total_views: Cumulative view count across all videos\n",
    "              - video_count: Total number of videos published\n",
    "    \"\"\"\n",
    "    request = youtube.channels().list(\n",
    "        part='snippet,statistics',\n",
    "        id=channel_id\n",
    "    )\n",
    "    \n",
    "    response = request.execute()\n",
    "    data = {}\n",
    "    \n",
    "    for item in response['items']:\n",
    "        data['channel_id'] = item['id']\n",
    "        data['channel_name'] = item['snippet']['title']\n",
    "        data['subscribers'] = item['statistics'].get('subscriberCount', 0)\n",
    "        data['total_views'] = item['statistics'].get('viewCount', 0)\n",
    "        data['video_count'] = item['statistics'].get('videoCount', 0)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def get_video_ids(youtube, channel_id, published_after=None):\n",
    "    \"\"\"\n",
    "    Retrieve a list of video IDs from a YouTube channel.\n",
    "    \n",
    "    OPTIMIZATION: Supports incremental fetching by filtering videos\n",
    "    published after a specific date, reducing API quota usage.\n",
    "    \n",
    "    Parameters:\n",
    "        youtube: The YouTube API client object\n",
    "        channel_id (str): The unique identifier for the YouTube channel\n",
    "        published_after (str): ISO 8601 timestamp to filter videos (optional)\n",
    "                              Only fetches videos published after this time\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of video ID strings\n",
    "    \n",
    "    Note:\n",
    "        Uses pagination to fetch all videos when needed.\n",
    "    \"\"\"\n",
    "    video_ids = []\n",
    "    next_page_token = None\n",
    "    \n",
    "    while True:\n",
    "        # Build request parameters\n",
    "        request_params = {\n",
    "            'part': 'id',\n",
    "            'channelId': channel_id,\n",
    "            'maxResults': 50,\n",
    "            'type': 'video',\n",
    "            'order': 'date'  # Order by date to get newest first\n",
    "        }\n",
    "        \n",
    "        # Add publishedAfter filter for incremental fetching\n",
    "        if published_after:\n",
    "            request_params['publishedAfter'] = published_after\n",
    "            logger.info(f\"Fetching videos published after: {published_after}\")\n",
    "        \n",
    "        # Add pagination token if available\n",
    "        if next_page_token:\n",
    "            request_params['pageToken'] = next_page_token\n",
    "        \n",
    "        request = youtube.search().list(**request_params)\n",
    "        response = request.execute()\n",
    "        \n",
    "        # Extract video IDs from response\n",
    "        for item in response['items']:\n",
    "            video_ids.append(item['id']['videoId'])\n",
    "        \n",
    "        # Check for more pages (only paginate for full fetches, not incremental)\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token or published_after:\n",
    "            # For incremental fetches, one page is usually enough\n",
    "            # For full fetches, continue pagination\n",
    "            if published_after:\n",
    "                break\n",
    "            elif not next_page_token:\n",
    "                break\n",
    "    \n",
    "    return video_ids\n",
    "\n",
    "\n",
    "def get_video_details(youtube, video_ids):\n",
    "    \"\"\"\n",
    "    Fetch detailed information for multiple YouTube videos.\n",
    "    \n",
    "    This function retrieves comprehensive metadata and statistics for a list\n",
    "    of video IDs. It processes videos in batches of 50 (API limit) to handle\n",
    "    large numbers of videos efficiently.\n",
    "    \n",
    "    Parameters:\n",
    "        youtube: The YouTube API client object\n",
    "        video_ids (list): List of video ID strings to fetch details for\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary contains:\n",
    "              - video_id: Unique video identifier\n",
    "              - title: Video title\n",
    "              - published_date: Date and time the video was published (ISO format)\n",
    "              - view_count: Number of views\n",
    "              - like_count: Number of likes\n",
    "              - comment_count: Number of comments\n",
    "              - duration: Video length in seconds (converted from ISO 8601)\n",
    "              - tags: Comma-separated list of video tags\n",
    "              - category_id: YouTube category identifier\n",
    "              - language: Video language code\n",
    "    \"\"\"\n",
    "    all_video_info = []\n",
    "    \n",
    "    # Skip if no videos to process\n",
    "    if not video_ids:\n",
    "        logger.info(\"No new videos to fetch details for\")\n",
    "        return all_video_info\n",
    "    \n",
    "    for i in range(0, len(video_ids), 50):\n",
    "        request = youtube.videos().list(\n",
    "            part='snippet,statistics,contentDetails',\n",
    "            id=','.join(video_ids[i:i+50])\n",
    "        )\n",
    "\n",
    "        response = request.execute()\n",
    "        \n",
    "        for item in response['items']:\n",
    "            video_info = {\n",
    "                'video_id': item['id'],\n",
    "                'title': item['snippet']['title'],\n",
    "                'published_date': item['snippet']['publishedAt'],\n",
    "                'view_count': item['statistics'].get('viewCount', 0),\n",
    "                'like_count': item['statistics'].get('likeCount', 0),\n",
    "                'comment_count': item['statistics'].get('commentCount', 0),\n",
    "                'duration': isodate.parse_duration(item['contentDetails']['duration']).total_seconds(),\n",
    "                'tags': ','.join(item['snippet'].get('tags', [])),\n",
    "                'category_id': item['snippet'].get('categoryId', ''),\n",
    "                'language': item['snippet'].get('defaultAudioLanguage', item['snippet'].get('defaultLanguage', ''))\n",
    "            }\n",
    "            all_video_info.append(video_info)\n",
    "    \n",
    "    return all_video_info\n",
    "\n",
    "\n",
    "def upload_to_s3_from_databricks(local_file_path, bucket):\n",
    "    \"\"\"\n",
    "    Upload files to S3 from Databricks using AWS credentials.\n",
    "    \n",
    "    In Databricks, AWS credentials can be configured at cluster level or\n",
    "    retrieved from secrets for S3 access.\n",
    "    \n",
    "    Parameters:\n",
    "        local_file_path (str): Local DBFS path to the file\n",
    "        s3_bucket (str): S3 bucket name\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import boto3\n",
    "        from botocore.exceptions import ClientError\n",
    "        \n",
    "        # Get AWS credentials from Databricks secrets\n",
    "        if IS_DATABRICKS:\n",
    "            try:\n",
    "                \n",
    "                s3_client = boto3.client(\n",
    "                    's3',\n",
    "                    aws_access_key_id = AWS_ACCESS_KEY_ID,\n",
    "                    aws_secret_access_key = AWS_SECRET_ACCESS_KEY\n",
    "                )\n",
    "            except:\n",
    "                # If secrets not configured, try using instance profile/IAM role\n",
    "                s3_client = boto3.client('s3')\n",
    "        else:\n",
    "            s3_client = boto3.client('s3')\n",
    "        \n",
    "        # Extract filename and create S3 key with hierarchical structure\n",
    "        filename = local_file_path.split('/')[-1]\n",
    "        now = datetime.datetime.now()\n",
    "        s3_key = f\"youtube-raw-data/{now.year}/{now.month:02d}/{now.day:02d}/{now.hour:02d}/{filename}\"\n",
    "        \n",
    "        # Upload file to S3\n",
    "        s3_client.upload_file(local_file_path, bucket, s3_key)\n",
    "        \n",
    "        s3_uri = f\"s3://{bucket}/{s3_key}\"\n",
    "        logger.info(f\"Successfully uploaded to {s3_uri}\")\n",
    "        print(f\" Uploaded: {s3_uri}\")\n",
    "        \n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        logger.error(f\"S3 upload failed: {e}\")\n",
    "        print(f\" Failed to upload {local_file_path}: {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error during S3 upload: {e}\")\n",
    "        print(f\" Error: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def save_to_delta_table(df, table_name, mode='append'):\n",
    "    \"\"\"\n",
    "    Save DataFrame to Delta Lake table (Databricks native format).\n",
    "    \n",
    "    Delta Lake provides ACID transactions, schema enforcement, and time travel.\n",
    "    This is the recommended storage format in Databricks.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pandas.DataFrame or pyspark.sql.DataFrame): Data to save\n",
    "        table_name (str): Name of the Delta table\n",
    "        mode (str): Save mode - 'overwrite', 'append', 'error', 'ignore'\n",
    "    \"\"\"\n",
    "    if IS_DATABRICKS:\n",
    "        try:\n",
    "            # Convert pandas DataFrame to Spark DataFrame if needed\n",
    "            if isinstance(df, pd.DataFrame):\n",
    "                spark_df = spark.createDataFrame(df)\n",
    "            else:\n",
    "                spark_df = df\n",
    "            \n",
    "            # Write to Delta table\n",
    "            spark_df.write \\\n",
    "                .format('delta') \\\n",
    "                .mode(mode) \\\n",
    "                .saveAsTable(table_name)\n",
    "            \n",
    "            logger.info(f\"Successfully saved to Delta table: {table_name}\")\n",
    "            print(f\" Data saved to Delta table: {table_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save to Delta table: {e}\")\n",
    "            print(f\" Error saving to Delta table: {e}\")\n",
    "    else:\n",
    "        logger.warning(\"Delta table save skipped - not in Databricks environment\")\n",
    "\n",
    "\n",
    "def main(channel_id, youtube, incremental=True):\n",
    "    \"\"\"\n",
    "    Main orchestration function to extract and save YouTube channel data.\n",
    "    \n",
    "    OPTIMIZATION: Supports incremental mode to only fetch new videos\n",
    "    since the last run, significantly reducing API calls.\n",
    "    \n",
    "    This function coordinates the entire data extraction pipeline:\n",
    "    1. Fetch channel-level statistics (always refreshed)\n",
    "    2. Load cache to get last fetch time\n",
    "    3. Retrieve only NEW video IDs (incremental) or all (full)\n",
    "    4. Get detailed information only for NEW videos\n",
    "    5. Update cache with new video IDs\n",
    "    6. Return data for consolidation\n",
    "    \n",
    "    Parameters:\n",
    "        channel_id (str): The unique identifier for the YouTube channel\n",
    "        youtube: The YouTube API client object\n",
    "        incremental (bool): If True, only fetch new videos since last run\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (channel_stats, video_details) - Dictionary and list containing\n",
    "               channel statistics and video details respectively\n",
    "    \"\"\"\n",
    "    # STEP 1: Get channel-level statistics (always fetch fresh stats)\n",
    "    logger.info(f\"Fetching channel statistics for {channel_id}...\")\n",
    "    print(f\"Fetching channel statistics for {channel_id}...\")\n",
    "    channel_stats = get_channel_stats(youtube, channel_id)\n",
    "    \n",
    "    # Display channel information\n",
    "    logger.info(f\"Channel: {channel_stats['channel_name']}\")\n",
    "    print(f\"Channel: {channel_stats['channel_name']}\")\n",
    "    logger.info(f\"Subscribers: {channel_stats['subscribers']}\")\n",
    "    print(f\"Subscribers: {channel_stats['subscribers']}\")\n",
    "    logger.info(f\"Total Views: {channel_stats['total_views']}\")\n",
    "    print(f\"Total Views: {channel_stats['total_views']}\")\n",
    "    logger.info(f\"Video Count: {channel_stats['video_count']}\\n\")\n",
    "    print(f\"Video Count: {channel_stats['video_count']}\\n\")\n",
    "    \n",
    "    # STEP 2: Load cache for incremental fetching\n",
    "    cache_data = load_channel_cache(channel_id)\n",
    "    cached_video_ids = set(cache_data.get('video_ids', []))\n",
    "    last_fetch_time = cache_data.get('last_fetch_time')\n",
    "    \n",
    "    # STEP 3: Get video IDs (incremental or full)\n",
    "    logger.info(\"Fetching video IDs...\")\n",
    "    print(\"Fetching video IDs...\")\n",
    "    \n",
    "    if incremental and last_fetch_time:\n",
    "        print(f\" INCREMENTAL MODE: Fetching videos since {last_fetch_time}\")\n",
    "        logger.info(f\"Incremental fetch - last run: {last_fetch_time}\")\n",
    "        video_ids = get_video_ids(youtube, channel_id, published_after=last_fetch_time)\n",
    "    else:\n",
    "        print(\" FULL MODE: Fetching all videos\")\n",
    "        logger.info(\"Full fetch - no previous cache or incremental disabled\")\n",
    "        video_ids = get_video_ids(youtube, channel_id)\n",
    "    \n",
    "    # STEP 4: Filter out already cached video IDs\n",
    "    new_video_ids = [vid for vid in video_ids if vid not in cached_video_ids]\n",
    "    \n",
    "    logger.info(f\"Found {len(video_ids)} videos from API\")\n",
    "    logger.info(f\"Already cached: {len(cached_video_ids)} videos\")\n",
    "    logger.info(f\"New videos to fetch: {len(new_video_ids)}\")\n",
    "    print(f\"Found {len(video_ids)} videos from API\")\n",
    "    print(f\"Already cached: {len(cached_video_ids)} videos\") \n",
    "    print(f\" New videos to fetch details: {len(new_video_ids)}\\n\")\n",
    "    \n",
    "    # STEP 5: Get detailed information only for NEW videos\n",
    "    logger.info(\"Fetching video details for new videos...\")\n",
    "    print(\"Fetching video details for new videos...\")\n",
    "    video_details = get_video_details(youtube, new_video_ids)\n",
    "    logger.info(f\"Retrieved details for {len(video_details)} new videos\\n\")\n",
    "    print(f\"Retrieved details for {len(video_details)} new videos\\n\")\n",
    "    \n",
    "    # STEP 6: Update cache with all video IDs (existing + new)\n",
    "    all_video_ids = list(cached_video_ids.union(set(new_video_ids)))\n",
    "    current_time = datetime.datetime.utcnow().isoformat() + 'Z'\n",
    "    save_channel_cache(channel_id, all_video_ids, current_time)\n",
    "    \n",
    "    # STEP 7: Enrich video details with channel information\n",
    "    for video in video_details:\n",
    "        video['channel_id'] = channel_stats['channel_id']\n",
    "        video['channel_name'] = channel_stats['channel_name']\n",
    "    \n",
    "    # Print API savings summary\n",
    "    if cached_video_ids:\n",
    "        saved_calls = len(cached_video_ids) // 50 + 1\n",
    "        print(f\" API SAVINGS: Skipped ~{saved_calls} API calls by using cache!\")\n",
    "    \n",
    "    return channel_stats, video_details\n",
    "\n",
    "\n",
    "def run_ingestion_pipeline(channel_csv_path=None, save_format='csv', upload_to_s3=True, incremental=True):\n",
    "    \"\"\"\n",
    "    Execute the complete YouTube data ingestion pipeline for Databricks.\n",
    "    \n",
    "    This is the main entry point for the Databricks notebook.\n",
    "    \n",
    "    OPTIMIZATION: Set incremental=True to only fetch new videos since last run.\n",
    "    This significantly reduces YouTube API quota usage.\n",
    "    \n",
    "    Parameters:\n",
    "        channel_csv_path (str): Path to CSV file with channel IDs (DBFS path)\n",
    "                               If None, uses default location\n",
    "        save_format (str): Output format - 'csv', 'delta', or 'both'\n",
    "        upload_to_s3 (bool): Whether to upload CSV files to S3\n",
    "        incremental (bool): If True, only fetch new videos since last run\n",
    "                           If False, fetch all videos (full refresh)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (channel_df, videos_df) - DataFrames with collected data\n",
    "    \"\"\"\n",
    "    # Setup directories\n",
    "    setup_dbfs_directories()\n",
    "    \n",
    "    # Get API credentials\n",
    "    API_KEY, S3_BUCKET = get_api_credentials()\n",
    "    \n",
    "    # Build YouTube API client\n",
    "    YOUTUBE_API_SERVICE_NAME = 'youtube'\n",
    "    YOUTUBE_API_VERSION = 'v3'\n",
    "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=API_KEY)\n",
    "    \n",
    "    # Load channel IDs from CSV\n",
    "    if channel_csv_path is None:\n",
    "        # Default location in DBFS\n",
    "        channel_csv_path = f'{DBFS_INPUT_PATH}/top10channelid.csv'\n",
    "    \n",
    "    logger.info(f\"Loading channel IDs from: {channel_csv_path}\")\n",
    "    print(f\"Loading channel IDs from: {channel_csv_path}\")\n",
    "    \n",
    "    # Print mode information\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    if incremental:\n",
    "        print(\" RUNNING IN INCREMENTAL MODE (Optimized)\")\n",
    "        print(\"   Only new videos since last run will be fetched\")\n",
    "    else:\n",
    "        print(\" RUNNING IN FULL REFRESH MODE\")\n",
    "        print(\"   All videos will be fetched (higher API usage)\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    try:\n",
    "        channelid_data = pd.read_csv(channel_csv_path)\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Channel ID file not found: {channel_csv_path}\")\n",
    "        print(f\" Error: Channel ID file not found at {channel_csv_path}\")\n",
    "        print(f\"Please upload your 'top10channelid.csv' file to {DBFS_INPUT_PATH}/\")\n",
    "        return None, None\n",
    "    \n",
    "    # Initialize data collection lists\n",
    "    all_channel_stats = []\n",
    "    all_video_details = []\n",
    "    \n",
    "    # Track API call savings\n",
    "    total_new_videos = 0\n",
    "    total_cached_videos = 0\n",
    "    \n",
    "    # Process each channel\n",
    "    for index, row in channelid_data.iterrows():\n",
    "        CHANNEL_ID = row['channel_id']\n",
    "        logger.info(f\"Processing channel ID: {CHANNEL_ID}\")\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Processing channel ID: {CHANNEL_ID}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        channel_stats, video_details = main(CHANNEL_ID, youtube, incremental=incremental)\n",
    "        \n",
    "        all_channel_stats.append(channel_stats)\n",
    "        all_video_details.extend(video_details)\n",
    "        total_new_videos += len(video_details)\n",
    "    \n",
    "    # Create DataFrames\n",
    "    print(\"\\nCreating final DataFrames...\")\n",
    "    logger.info(\"Creating final DataFrames...\")\n",
    "    \n",
    "    channel_df = pd.DataFrame(all_channel_stats)\n",
    "    videos_df = pd.DataFrame(all_video_details)\n",
    "    \n",
    "    # ---- FIX DATA TYPES BEFORE SPARK ----\n",
    "    if len(videos_df) > 0:\n",
    "        numeric_cols = [\n",
    "            \"view_count\",\n",
    "            \"like_count\",\n",
    "            \"comment_count\",\n",
    "            \"duration\"\n",
    "        ]\n",
    "\n",
    "        for col in numeric_cols:\n",
    "            videos_df[col] = pd.to_numeric(videos_df[col], errors=\"coerce\").fillna(0).astype(\"int64\")\n",
    "\n",
    "    channel_numeric_cols = [\n",
    "        \"subscribers\",\n",
    "        \"total_views\",\n",
    "        \"video_count\"\n",
    "    ]\n",
    "\n",
    "    for col in channel_numeric_cols:\n",
    "        channel_df[col] = pd.to_numeric(channel_df[col], errors=\"coerce\").fillna(0).astype(\"int64\")\n",
    "\n",
    "    # Generate timestamp\n",
    "    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Save to CSV if requested (only if we have new data)\n",
    "    if save_format in ['csv', 'both']:\n",
    "        channel_filename = f'{DBFS_OUTPUT_PATH}/channel_stats_{timestamp}.csv'\n",
    "        videos_filename = f'{DBFS_OUTPUT_PATH}/video_details_{timestamp}.csv'\n",
    "        \n",
    "        channel_df.to_csv(channel_filename, index=False)\n",
    "        videos_df.to_csv(videos_filename, index=False)\n",
    "        \n",
    "        print(f\"\\nCSV files saved to:\")\n",
    "        print(f\"  - {channel_filename}\")\n",
    "        print(f\"  - {videos_filename}\")\n",
    "        logger.info(f\"CSV files saved to {DBFS_OUTPUT_PATH}\")\n",
    "        \n",
    "        # Upload to S3 if requested\n",
    "        if upload_to_s3 and S3_BUCKET:\n",
    "            print(f\"\\nUploading files to S3 bucket: {S3_BUCKET}...\")\n",
    "            logger.info(f\"Starting S3 upload to bucket: {S3_BUCKET}\")\n",
    "            \n",
    "            success_channel = upload_to_s3_from_databricks(channel_filename, S3_BUCKET)\n",
    "            success_videos = upload_to_s3_from_databricks(videos_filename, S3_BUCKET)\n",
    "            \n",
    "            if success_channel and success_videos:\n",
    "                print(\"\\n All files successfully uploaded to S3!\")\n",
    "                logger.info(\"All files successfully uploaded to S3\")\n",
    "            else:\n",
    "                print(\"\\n Some files failed to upload to S3. Check logs for details.\")\n",
    "                logger.warning(\"Some S3 uploads failed\")\n",
    "        elif upload_to_s3 and not S3_BUCKET:\n",
    "            print(\"\\n S3 bucket not configured. Skipping S3 upload.\")\n",
    "            logger.warning(\"S3 bucket not configured, skipping S3 upload\")\n",
    "    \n",
    "    # Save to Delta tables if requested\n",
    "    if save_format in ['delta', 'both']:\n",
    "        save_to_delta_table(channel_df, f'youtube_channel_stats_{timestamp}', mode='overwrite')\n",
    "        if len(videos_df) > 0:\n",
    "            save_to_delta_table(videos_df, f'youtube_video_details_{timestamp}', mode='overwrite')\n",
    "        else:\n",
    "            print(\" No new videos to save to Delta table\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" Pipeline execution completed successfully!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\" SUMMARY:\")\n",
    "    print(f\"   - Channels processed: {len(channel_df)}\")\n",
    "    print(f\"   - New videos fetched: {total_new_videos}\")\n",
    "    if incremental:\n",
    "        print(f\"   - Mode: INCREMENTAL (optimized)\")\n",
    "    else:\n",
    "        print(f\"   - Mode: FULL REFRESH\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    logger.info(\"Pipeline execution completed\")\n",
    "    \n",
    "    return channel_df, videos_df\n",
    "\n",
    "\n",
    "def clear_cache(channel_id=None):\n",
    "    \"\"\"\n",
    "    Clear the video cache for a specific channel or all channels.\n",
    "    \n",
    "    Use this when you want to do a full refresh of video data.\n",
    "    \n",
    "    Parameters:\n",
    "        channel_id (str): Specific channel ID to clear cache for.\n",
    "                         If None, clears all channel caches.\n",
    "    \"\"\"\n",
    "    if channel_id:\n",
    "        cache_file = get_cache_file_path(channel_id)\n",
    "        try:\n",
    "            import os\n",
    "            os.remove(cache_file)\n",
    "            print(f\" Cleared cache for channel: {channel_id}\")\n",
    "            logger.info(f\"Cleared cache for channel: {channel_id}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\" No cache found for channel: {channel_id}\")\n",
    "        except Exception as e:\n",
    "            print(f\" Error clearing cache: {e}\")\n",
    "    else:\n",
    "        # Clear all caches\n",
    "        try:\n",
    "            if IS_DATABRICKS:\n",
    "                dbfs_path = DBFS_CACHE_PATH.replace('/dbfs', 'dbfs:')\n",
    "                files = dbutils.fs.ls(dbfs_path)\n",
    "                for f in files:\n",
    "                    if f.name.endswith('_cache.json'):\n",
    "                        dbutils.fs.rm(f.path)\n",
    "                print(f\" Cleared all channel caches\")\n",
    "            else:\n",
    "                import os\n",
    "                import glob\n",
    "                for cache_file in glob.glob(f'{DBFS_CACHE_PATH}/*_cache.json'):\n",
    "                    os.remove(cache_file)\n",
    "                print(f\" Cleared all channel caches\")\n",
    "        except Exception as e:\n",
    "            print(f\" Error clearing caches: {e}\")\n",
    "\n",
    "\n",
    "# ===== DATABRICKS NOTEBOOK EXECUTION =====\n",
    "# To run this in a Databricks notebook, execute:\n",
    "#\n",
    "# INCREMENTAL MODE (recommended for daily runs - saves API quota):\n",
    "# channel_df, videos_df = run_ingestion_pipeline(\n",
    "#     save_format='both',\n",
    "#     upload_to_s3=True,\n",
    "#     incremental=True  # Only fetch new videos\n",
    "# )\n",
    "#\n",
    "# FULL REFRESH MODE (use when you need all data):\n",
    "# channel_df, videos_df = run_ingestion_pipeline(\n",
    "#     save_format='both',\n",
    "#     upload_to_s3=True,\n",
    "#     incremental=False  # Fetch all videos\n",
    "# )\n",
    "#\n",
    "# TO CLEAR CACHE (before full refresh):\n",
    "# clear_cache()  # Clear all caches\n",
    "# clear_cache('UCxxxxxxx')  # Clear specific channel\n",
    "\n",
    "channel_df, videos_df = run_ingestion_pipeline(\n",
    "    save_format='both',  # Save as both CSV and Delta tables\n",
    "    upload_to_s3=True,\n",
    "    incremental=True  # Set to False for full refresh\n",
    ")\n",
    "# display(channel_df)\n",
    "# display(videos_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": {
    "autoRunOnWidgetChange": "no-auto-run"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ingestions_databricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
