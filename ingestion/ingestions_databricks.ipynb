{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fde030e-3f1f-4e7e-a586-36c481d9ac0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83aa5f0b-f6d1-4471-9b15-fb46eaaa8241",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 1"
    }
   },
   "outputs": [],
   "source": [
    "# ===== CREATE JOB WIDGETS =====\n",
    "# Not needed for production\n",
    "# dbutils.widgets.text(\"YOUTUBE_API_KEY\", \"\")\n",
    "# dbutils.widgets.text(\"S3_BUCKET\", \"\")\n",
    "# dbutils.widgets.text(\"CHANNEL_CSV_PATH\", \"\")\n",
    "# dbutils.widgets.text(\"SAVE_FORMAT\", \"csv\")\n",
    "# dbutils.widgets.text(\"UPLOAD_TO_S3\", \"true\")\n",
    "# dbutils.widgets.text(\"AWS_ACCESS_KEY_ID\", \"\")\n",
    "# dbutils.widgets.text(\"AWS_SECRET_ACCESS_KEY\", \"\")\n",
    "\n",
    "# ===== JOB PARAMETERS =====\n",
    "YOUTUBE_API_KEY = dbutils.widgets.get(\"YOUTUBE_API_KEY\")\n",
    "S3_BUCKET = dbutils.widgets.get(\"S3_BUCKET\")\n",
    "CHANNEL_CSV_PATH = dbutils.widgets.get(\"CHANNEL_CSV_PATH\")\n",
    "SAVE_FORMAT = dbutils.widgets.get(\"SAVE_FORMAT\")\n",
    "UPLOAD_TO_S3 = dbutils.widgets.get(\"UPLOAD_TO_S3\").lower() == \"true\"\n",
    "AWS_ACCESS_KEY_ID = dbutils.widgets.get(\"AWS_ACCESS_KEY_ID\")\n",
    "AWS_SECRET_ACCESS_KEY = dbutils.widgets.get(\"AWS_SECRET_ACCESS_KEY\")\n",
    "\n",
    "\n",
    "'''\n",
    "YouTube Analytics Data Ingestion Script - Databricks Version\n",
    "\n",
    "This script extracts comprehensive data from YouTube channels using the YouTube Data API v3.\n",
    "Optimized for Databricks environment with DBFS integration.\n",
    "\n",
    "OPTIMIZATION FEATURES:\n",
    "- Incremental fetching: Only fetches new videos since last run\n",
    "- Video ID caching: Stores previously fetched video IDs to avoid duplicates\n",
    "- Reduced API calls: Skips re-fetching details for already processed videos\n",
    "\n",
    "Data Extracted:\n",
    "Channel-level:\n",
    "    - Channel ID\n",
    "    - Channel name\n",
    "    - Subscribers count\n",
    "    - Total views\n",
    "    - Video count\n",
    "\n",
    "Video-level:\n",
    "    - Video ID\n",
    "    - Title\n",
    "    - Published date\n",
    "    - View count\n",
    "    - Like count\n",
    "    - Comment count\n",
    "    - Duration\n",
    "\n",
    "Optional (bonus):\n",
    "    - Tags\n",
    "    - Category\n",
    "    - Language\n",
    "'''\n",
    "\n",
    "# Import required libraries\n",
    "from googleapiclient.discovery import build  # Google API client for YouTube API\n",
    "import pandas as pd  # Data manipulation and CSV export\n",
    "import isodate  # Parse ISO 8601 duration format (e.g., PT4M13S)\n",
    "import datetime  # Timestamp generation for file naming\n",
    "import logging\n",
    "import json  # For caching metadata\n",
    "\n",
    "\n",
    "# ===== DATABRICKS CONFIGURATION =====\n",
    "# In Databricks, use dbutils for secrets management and file operations\n",
    "# Secrets should be stored in Databricks Secret Scopes\n",
    "# Example: dbutils.secrets.put(scope=\"youtube-api\", key=\"api-key\", value=\"YOUR_API_KEY\")\n",
    "\n",
    "try:\n",
    "    # Try to get dbutils (available in Databricks notebooks)\n",
    "    from pyspark.dbutils import DBUtils\n",
    "    dbutils = DBUtils(spark)\n",
    "    IS_DATABRICKS = True\n",
    "except ImportError:\n",
    "    # Fallback for testing outside Databricks\n",
    "    IS_DATABRICKS = False\n",
    "    print(\"Warning: Not running in Databricks environment. Using fallback configuration.\")\n",
    "\n",
    "\n",
    "# ===== LOGGING CONFIGURATION =====\n",
    "# Set up logging for Databricks environment\n",
    "# Logs will be visible in the notebook output and cluster logs\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# ===== DBFS PATH CONFIGURATION =====\n",
    "# Databricks File System (DBFS) paths for storing data\n",
    "# Use /dbfs/ prefix for local file system access or dbfs:/ for Spark operations\n",
    "DBFS_BASE_PATH = '/Workspace/Users/shamanthkrishna0@gmail.com/youtube-analytics-data-pipeline'\n",
    "DBFS_OUTPUT_PATH = f'{DBFS_BASE_PATH}/Output'\n",
    "DBFS_LOGS_PATH = f'{DBFS_BASE_PATH}/Logs'\n",
    "DBFS_INPUT_PATH = f'{DBFS_BASE_PATH}/input'\n",
    "DBFS_CACHE_PATH = f'{DBFS_BASE_PATH}/cache'  # New: Cache directory for incremental fetching\n",
    "\n",
    "\n",
    "def setup_dbfs_directories():\n",
    "    \"\"\"\n",
    "    Create necessary DBFS directories for the pipeline.\n",
    "    Uses dbutils.fs for Databricks-native file operations.\n",
    "    \"\"\"\n",
    "    if IS_DATABRICKS:\n",
    "        # Create directories using dbutils\n",
    "        for path in [DBFS_OUTPUT_PATH, DBFS_LOGS_PATH, DBFS_INPUT_PATH, DBFS_CACHE_PATH]:\n",
    "            dbfs_path = path.replace('/dbfs', 'dbfs:')\n",
    "            try:\n",
    "                dbutils.fs.mkdirs(dbfs_path)\n",
    "                logger.info(f\"Created/verified directory: {dbfs_path}\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not create directory {dbfs_path}: {e}\")\n",
    "    else:\n",
    "        # Fallback for local testing\n",
    "        import os\n",
    "        for path in [DBFS_OUTPUT_PATH, DBFS_LOGS_PATH, DBFS_INPUT_PATH, DBFS_CACHE_PATH]:\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "# ===== CACHING FUNCTIONS FOR INCREMENTAL FETCHING =====\n",
    "def get_cache_file_path(channel_id):\n",
    "    \"\"\"Get the cache file path for a specific channel.\"\"\"\n",
    "    return f'{DBFS_CACHE_PATH}/channel_{channel_id}_cache.json'\n",
    "\n",
    "\n",
    "def load_channel_cache(channel_id):\n",
    "    \"\"\"\n",
    "    Load cached data for a channel including:\n",
    "    - Last fetch timestamp\n",
    "    - Previously fetched video IDs\n",
    "    \n",
    "    Returns:\n",
    "        dict: Cache data with 'last_fetch_time' and 'video_ids' keys\n",
    "              Returns empty cache if file doesn't exist\n",
    "    \"\"\"\n",
    "    cache_file = get_cache_file_path(channel_id)\n",
    "    try:\n",
    "        with open(cache_file, 'r') as f:\n",
    "            cache_data = json.load(f)\n",
    "            logger.info(f\"Loaded cache for channel {channel_id}: {len(cache_data.get('video_ids', []))} cached videos\")\n",
    "            return cache_data\n",
    "    except FileNotFoundError:\n",
    "        logger.info(f\"No cache found for channel {channel_id}, will fetch all videos\")\n",
    "        return {'last_fetch_time': None, 'video_ids': []}\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error loading cache for {channel_id}: {e}\")\n",
    "        return {'last_fetch_time': None, 'video_ids': []}\n",
    "\n",
    "\n",
    "def save_channel_cache(channel_id, video_ids, fetch_time=None):\n",
    "    \"\"\"\n",
    "    Save cache data for a channel.\n",
    "    \n",
    "    Parameters:\n",
    "        channel_id (str): The channel ID\n",
    "        video_ids (list): List of all video IDs fetched (cumulative)\n",
    "        fetch_time (str): ISO format timestamp of this fetch\n",
    "    \"\"\"\n",
    "    cache_file = get_cache_file_path(channel_id)\n",
    "    if fetch_time is None:\n",
    "        fetch_time = datetime.datetime.utcnow().isoformat() + 'Z'\n",
    "    \n",
    "    cache_data = {\n",
    "        'last_fetch_time': fetch_time,\n",
    "        'video_ids': list(set(video_ids)),  # Deduplicate\n",
    "        'updated_at': datetime.datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with open(cache_file, 'w') as f:\n",
    "            json.dump(cache_data, f, indent=2)\n",
    "        logger.info(f\"Saved cache for channel {channel_id}: {len(cache_data['video_ids'])} total videos\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving cache for {channel_id}: {e}\")\n",
    "\n",
    "\n",
    "def get_api_credentials():\n",
    "    \"\"\"\n",
    "    Retrieve API credentials from Databricks secrets.\n",
    "    \n",
    "    In Databricks, create a secret scope and add your API key:\n",
    "    - Scope name: 'youtube-api' (or customize)\n",
    "    - Key name: 'api-key'\n",
    "    \n",
    "    Command to create secret scope (run in Databricks notebook):\n",
    "    dbutils.secrets.help()\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (API_KEY, S3_BUCKET_NAME)\n",
    "    \"\"\"\n",
    "    if IS_DATABRICKS:\n",
    "        try:\n",
    "            # Retrieve API key from Databricks secrets\n",
    "            # Replace 'youtube-api' with your actual secret scope name\n",
    "            # Optional: Get S3 bucket name from secrets\n",
    "            API_KEY = YOUTUBE_API_KEY\n",
    "            bucket = S3_BUCKET\n",
    "            \n",
    "            logger.info(\"Successfully retrieved credentials from Databricks secrets\")\n",
    "            return API_KEY, bucket\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error retrieving secrets: {e}\")\n",
    "            raise ValueError(\"Failed to retrieve API credentials from Databricks secrets. \"\n",
    "                           \"Please configure the 'youtube-api' secret scope.\")\n",
    "    else:\n",
    "        # Fallback for local testing\n",
    "        import os\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv()\n",
    "        return API_KEY, S3_BUCKET\n",
    "\n",
    "\n",
    "def get_channel_stats(youtube, channel_id):\n",
    "    \"\"\"\n",
    "    Fetch channel-level statistics from YouTube API.\n",
    "    \n",
    "    This function retrieves basic information and statistics for a YouTube channel,\n",
    "    including subscriber count, total views, and number of videos published.\n",
    "    \n",
    "    Parameters:\n",
    "        youtube: The YouTube API client object\n",
    "        channel_id (str): The unique identifier for the YouTube channel\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing channel statistics with keys:\n",
    "              - channel_id: The channel's unique identifier\n",
    "              - channel_name: The display name of the channel\n",
    "              - subscribers: Total subscriber count\n",
    "              - total_views: Cumulative view count across all videos\n",
    "              - video_count: Total number of videos published\n",
    "    \"\"\"\n",
    "    request = youtube.channels().list(\n",
    "        part='snippet,statistics',\n",
    "        id=channel_id\n",
    "    )\n",
    "    \n",
    "    response = request.execute()\n",
    "    data = {}\n",
    "    \n",
    "    for item in response['items']:\n",
    "        data['channel_id'] = item['id']\n",
    "        data['channel_name'] = item['snippet']['title']\n",
    "        data['subscribers'] = item['statistics'].get('subscriberCount', 0)\n",
    "        data['total_views'] = item['statistics'].get('viewCount', 0)\n",
    "        data['video_count'] = item['statistics'].get('videoCount', 0)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def get_video_ids(youtube, channel_id, published_after=None):\n",
    "    \"\"\"\n",
    "    Retrieve a list of video IDs from a YouTube channel.\n",
    "    \n",
    "    OPTIMIZATION: Supports incremental fetching by filtering videos\n",
    "    published after a specific date, reducing API quota usage.\n",
    "    \n",
    "    Parameters:\n",
    "        youtube: The YouTube API client object\n",
    "        channel_id (str): The unique identifier for the YouTube channel\n",
    "        published_after (str): ISO 8601 timestamp to filter videos (optional)\n",
    "                              Only fetches videos published after this time\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of video ID strings\n",
    "    \n",
    "    Note:\n",
    "        Uses pagination to fetch all videos when needed.\n",
    "    \"\"\"\n",
    "    video_ids = []\n",
    "    next_page_token = None\n",
    "    \n",
    "    while True:\n",
    "        # Build request parameters\n",
    "        request_params = {\n",
    "            'part': 'id',\n",
    "            'channelId': channel_id,\n",
    "            'maxResults': 50,\n",
    "            'type': 'video',\n",
    "            'order': 'date'  # Order by date to get newest first\n",
    "        }\n",
    "        \n",
    "        # Add publishedAfter filter for incremental fetching\n",
    "        if published_after:\n",
    "            request_params['publishedAfter'] = published_after\n",
    "            logger.info(f\"Fetching videos published after: {published_after}\")\n",
    "        \n",
    "        # Add pagination token if available\n",
    "        if next_page_token:\n",
    "            request_params['pageToken'] = next_page_token\n",
    "        \n",
    "        request = youtube.search().list(**request_params)\n",
    "        response = request.execute()\n",
    "        \n",
    "        # Extract video IDs from response\n",
    "        for item in response['items']:\n",
    "            video_ids.append(item['id']['videoId'])\n",
    "        \n",
    "        # Check for more pages (only paginate for full fetches, not incremental)\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token or published_after:\n",
    "            # For incremental fetches, one page is usually enough\n",
    "            # For full fetches, continue pagination\n",
    "            if published_after:\n",
    "                break\n",
    "            elif not next_page_token:\n",
    "                break\n",
    "    \n",
    "    return video_ids\n",
    "\n",
    "\n",
    "def get_video_details(youtube, video_ids):\n",
    "    \"\"\"\n",
    "    Fetch detailed information for multiple YouTube videos.\n",
    "    \n",
    "    This function retrieves comprehensive metadata and statistics for a list\n",
    "    of video IDs. It processes videos in batches of 50 (API limit) to handle\n",
    "    large numbers of videos efficiently.\n",
    "    \n",
    "    Parameters:\n",
    "        youtube: The YouTube API client object\n",
    "        video_ids (list): List of video ID strings to fetch details for\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary contains:\n",
    "              - video_id: Unique video identifier\n",
    "              - title: Video title\n",
    "              - published_date: Date and time the video was published (ISO format)\n",
    "              - view_count: Number of views\n",
    "              - like_count: Number of likes\n",
    "              - comment_count: Number of comments\n",
    "              - duration: Video length in seconds (converted from ISO 8601)\n",
    "              - tags: Comma-separated list of video tags\n",
    "              - category_id: YouTube category identifier\n",
    "              - language: Video language code\n",
    "    \"\"\"\n",
    "    all_video_info = []\n",
    "    \n",
    "    # Skip if no videos to process\n",
    "    if not video_ids:\n",
    "        logger.info(\"No new videos to fetch details for\")\n",
    "        return all_video_info\n",
    "    \n",
    "    for i in range(0, len(video_ids), 50):\n",
    "        request = youtube.videos().list(\n",
    "            part='snippet,statistics,contentDetails',\n",
    "            id=','.join(video_ids[i:i+50])\n",
    "        )\n",
    "\n",
    "        response = request.execute()\n",
    "        \n",
    "        for item in response['items']:\n",
    "            video_info = {\n",
    "                'video_id': item['id'],\n",
    "                'title': item['snippet']['title'],\n",
    "                'published_date': item['snippet']['publishedAt'],\n",
    "                'view_count': item['statistics'].get('viewCount', 0),\n",
    "                'like_count': item['statistics'].get('likeCount', 0),\n",
    "                'comment_count': item['statistics'].get('commentCount', 0),\n",
    "                'duration': isodate.parse_duration(item['contentDetails']['duration']).total_seconds(),\n",
    "                'tags': ','.join(item['snippet'].get('tags', [])),\n",
    "                'category_id': item['snippet'].get('categoryId', ''),\n",
    "                'language': item['snippet'].get('defaultAudioLanguage', item['snippet'].get('defaultLanguage', ''))\n",
    "            }\n",
    "            all_video_info.append(video_info)\n",
    "    \n",
    "    return all_video_info\n",
    "\n",
    "\n",
    "def upload_to_s3_from_databricks(local_file_path, bucket):\n",
    "    \"\"\"\n",
    "    Upload files to S3 from Databricks using AWS credentials.\n",
    "    \n",
    "    In Databricks, AWS credentials can be configured at cluster level or\n",
    "    retrieved from secrets for S3 access.\n",
    "    \n",
    "    Parameters:\n",
    "        local_file_path (str): Local DBFS path to the file\n",
    "        s3_bucket (str): S3 bucket name\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import boto3\n",
    "        from botocore.exceptions import ClientError\n",
    "        \n",
    "        # Get AWS credentials from Databricks secrets\n",
    "        if IS_DATABRICKS:\n",
    "            try:\n",
    "                \n",
    "                s3_client = boto3.client(\n",
    "                    's3',\n",
    "                    aws_access_key_id = AWS_ACCESS_KEY_ID,\n",
    "                    aws_secret_access_key = AWS_SECRET_ACCESS_KEY\n",
    "                )\n",
    "            except:\n",
    "                # If secrets not configured, try using instance profile/IAM role\n",
    "                s3_client = boto3.client('s3')\n",
    "        else:\n",
    "            s3_client = boto3.client('s3')\n",
    "        \n",
    "        # Extract filename and create S3 key with hierarchical structure\n",
    "        filename = local_file_path.split('/')[-1]\n",
    "        now = datetime.datetime.now()\n",
    "        s3_key = f\"youtube-raw-data/{now.year}/{now.month:02d}/{now.day:02d}/{now.hour:02d}/{filename}\"\n",
    "        \n",
    "        # Upload file to S3\n",
    "        s3_client.upload_file(local_file_path, bucket, s3_key)\n",
    "        \n",
    "        s3_uri = f\"s3://{bucket}/{s3_key}\"\n",
    "        logger.info(f\"Successfully uploaded to {s3_uri}\")\n",
    "        print(f\"‚úì Uploaded: {s3_uri}\")\n",
    "        \n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        logger.error(f\"S3 upload failed: {e}\")\n",
    "        print(f\"‚úó Failed to upload {local_file_path}: {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error during S3 upload: {e}\")\n",
    "        print(f\"‚úó Error: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def save_to_delta_table(df, table_name, mode='append'):\n",
    "    \"\"\"\n",
    "    Save DataFrame to Delta Lake table (Databricks native format).\n",
    "    \n",
    "    Delta Lake provides ACID transactions, schema enforcement, and time travel.\n",
    "    This is the recommended storage format in Databricks.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pandas.DataFrame or pyspark.sql.DataFrame): Data to save\n",
    "        table_name (str): Name of the Delta table\n",
    "        mode (str): Save mode - 'overwrite', 'append', 'error', 'ignore'\n",
    "    \"\"\"\n",
    "    if IS_DATABRICKS:\n",
    "        try:\n",
    "            # Convert pandas DataFrame to Spark DataFrame if needed\n",
    "            if isinstance(df, pd.DataFrame):\n",
    "                spark_df = spark.createDataFrame(df)\n",
    "            else:\n",
    "                spark_df = df\n",
    "            \n",
    "            # Write to Delta table\n",
    "            spark_df.write \\\n",
    "                .format('delta') \\\n",
    "                .mode(mode) \\\n",
    "                .saveAsTable(table_name)\n",
    "            \n",
    "            logger.info(f\"Successfully saved to Delta table: {table_name}\")\n",
    "            print(f\"‚úì Data saved to Delta table: {table_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save to Delta table: {e}\")\n",
    "            print(f\"‚úó Error saving to Delta table: {e}\")\n",
    "    else:\n",
    "        logger.warning(\"Delta table save skipped - not in Databricks environment\")\n",
    "\n",
    "\n",
    "def main(channel_id, youtube, incremental=True):\n",
    "    \"\"\"\n",
    "    Main orchestration function to extract and save YouTube channel data.\n",
    "    \n",
    "    OPTIMIZATION: Supports incremental mode to only fetch new videos\n",
    "    since the last run, significantly reducing API calls.\n",
    "    \n",
    "    This function coordinates the entire data extraction pipeline:\n",
    "    1. Fetch channel-level statistics (always refreshed)\n",
    "    2. Load cache to get last fetch time\n",
    "    3. Retrieve only NEW video IDs (incremental) or all (full)\n",
    "    4. Get detailed information only for NEW videos\n",
    "    5. Update cache with new video IDs\n",
    "    6. Return data for consolidation\n",
    "    \n",
    "    Parameters:\n",
    "        channel_id (str): The unique identifier for the YouTube channel\n",
    "        youtube: The YouTube API client object\n",
    "        incremental (bool): If True, only fetch new videos since last run\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (channel_stats, video_details) - Dictionary and list containing\n",
    "               channel statistics and video details respectively\n",
    "    \"\"\"\n",
    "    # STEP 1: Get channel-level statistics (always fetch fresh stats)\n",
    "    logger.info(f\"Fetching channel statistics for {channel_id}...\")\n",
    "    print(f\"Fetching channel statistics for {channel_id}...\")\n",
    "    channel_stats = get_channel_stats(youtube, channel_id)\n",
    "    \n",
    "    # Display channel information\n",
    "    logger.info(f\"Channel: {channel_stats['channel_name']}\")\n",
    "    print(f\"Channel: {channel_stats['channel_name']}\")\n",
    "    logger.info(f\"Subscribers: {channel_stats['subscribers']}\")\n",
    "    print(f\"Subscribers: {channel_stats['subscribers']}\")\n",
    "    logger.info(f\"Total Views: {channel_stats['total_views']}\")\n",
    "    print(f\"Total Views: {channel_stats['total_views']}\")\n",
    "    logger.info(f\"Video Count: {channel_stats['video_count']}\\n\")\n",
    "    print(f\"Video Count: {channel_stats['video_count']}\\n\")\n",
    "    \n",
    "    # STEP 2: Load cache for incremental fetching\n",
    "    cache_data = load_channel_cache(channel_id)\n",
    "    cached_video_ids = set(cache_data.get('video_ids', []))\n",
    "    last_fetch_time = cache_data.get('last_fetch_time')\n",
    "    \n",
    "    # STEP 3: Get video IDs (incremental or full)\n",
    "    logger.info(\"Fetching video IDs...\")\n",
    "    print(\"Fetching video IDs...\")\n",
    "    \n",
    "    if incremental and last_fetch_time:\n",
    "        print(f\"üìä INCREMENTAL MODE: Fetching videos since {last_fetch_time}\")\n",
    "        logger.info(f\"Incremental fetch - last run: {last_fetch_time}\")\n",
    "        video_ids = get_video_ids(youtube, channel_id, published_after=last_fetch_time)\n",
    "    else:\n",
    "        print(\"üìä FULL MODE: Fetching all videos\")\n",
    "        logger.info(\"Full fetch - no previous cache or incremental disabled\")\n",
    "        video_ids = get_video_ids(youtube, channel_id)\n",
    "    \n",
    "    # STEP 4: Filter out already cached video IDs\n",
    "    new_video_ids = [vid for vid in video_ids if vid not in cached_video_ids]\n",
    "    \n",
    "    logger.info(f\"Found {len(video_ids)} videos from API\")\n",
    "    logger.info(f\"Already cached: {len(cached_video_ids)} videos\")\n",
    "    logger.info(f\"New videos to fetch: {len(new_video_ids)}\")\n",
    "    print(f\"Found {len(video_ids)} videos from API\")\n",
    "    print(f\"Already cached: {len(cached_video_ids)} videos\") \n",
    "    print(f\"üÜï New videos to fetch details: {len(new_video_ids)}\\n\")\n",
    "    \n",
    "    # STEP 5: Get detailed information only for NEW videos\n",
    "    logger.info(\"Fetching video details for new videos...\")\n",
    "    print(\"Fetching video details for new videos...\")\n",
    "    video_details = get_video_details(youtube, new_video_ids)\n",
    "    logger.info(f\"Retrieved details for {len(video_details)} new videos\\n\")\n",
    "    print(f\"Retrieved details for {len(video_details)} new videos\\n\")\n",
    "    \n",
    "    # STEP 6: Update cache with all video IDs (existing + new)\n",
    "    all_video_ids = list(cached_video_ids.union(set(new_video_ids)))\n",
    "    current_time = datetime.datetime.utcnow().isoformat() + 'Z'\n",
    "    save_channel_cache(channel_id, all_video_ids, current_time)\n",
    "    \n",
    "    # STEP 7: Enrich video details with channel information\n",
    "    for video in video_details:\n",
    "        video['channel_id'] = channel_stats['channel_id']\n",
    "        video['channel_name'] = channel_stats['channel_name']\n",
    "    \n",
    "    # Print API savings summary\n",
    "    if cached_video_ids:\n",
    "        saved_calls = len(cached_video_ids) // 50 + 1\n",
    "        print(f\"üí∞ API SAVINGS: Skipped ~{saved_calls} API calls by using cache!\")\n",
    "    \n",
    "    return channel_stats, video_details\n",
    "\n",
    "\n",
    "def run_ingestion_pipeline(channel_csv_path=None, save_format='csv', upload_to_s3=True, incremental=True):\n",
    "    \"\"\"\n",
    "    Execute the complete YouTube data ingestion pipeline for Databricks.\n",
    "    \n",
    "    This is the main entry point for the Databricks notebook.\n",
    "    \n",
    "    OPTIMIZATION: Set incremental=True to only fetch new videos since last run.\n",
    "    This significantly reduces YouTube API quota usage.\n",
    "    \n",
    "    Parameters:\n",
    "        channel_csv_path (str): Path to CSV file with channel IDs (DBFS path)\n",
    "                               If None, uses default location\n",
    "        save_format (str): Output format - 'csv', 'delta', or 'both'\n",
    "        upload_to_s3 (bool): Whether to upload CSV files to S3\n",
    "        incremental (bool): If True, only fetch new videos since last run\n",
    "                           If False, fetch all videos (full refresh)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (channel_df, videos_df) - DataFrames with collected data\n",
    "    \"\"\"\n",
    "    # Setup directories\n",
    "    setup_dbfs_directories()\n",
    "    \n",
    "    # Get API credentials\n",
    "    API_KEY, S3_BUCKET = get_api_credentials()\n",
    "    \n",
    "    # Build YouTube API client\n",
    "    YOUTUBE_API_SERVICE_NAME = 'youtube'\n",
    "    YOUTUBE_API_VERSION = 'v3'\n",
    "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=API_KEY)\n",
    "    \n",
    "    # Load channel IDs from CSV\n",
    "    if channel_csv_path is None:\n",
    "        # Default location in DBFS\n",
    "        channel_csv_path = f'{DBFS_INPUT_PATH}/top10channelid.csv'\n",
    "    \n",
    "    logger.info(f\"Loading channel IDs from: {channel_csv_path}\")\n",
    "    print(f\"Loading channel IDs from: {channel_csv_path}\")\n",
    "    \n",
    "    # Print mode information\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    if incremental:\n",
    "        print(\"üöÄ RUNNING IN INCREMENTAL MODE (Optimized)\")\n",
    "        print(\"   Only new videos since last run will be fetched\")\n",
    "    else:\n",
    "        print(\"üîÑ RUNNING IN FULL REFRESH MODE\")\n",
    "        print(\"   All videos will be fetched (higher API usage)\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    try:\n",
    "        channelid_data = pd.read_csv(channel_csv_path)\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Channel ID file not found: {channel_csv_path}\")\n",
    "        print(f\"‚úó Error: Channel ID file not found at {channel_csv_path}\")\n",
    "        print(f\"Please upload your 'top10channelid.csv' file to {DBFS_INPUT_PATH}/\")\n",
    "        return None, None\n",
    "    \n",
    "    # Initialize data collection lists\n",
    "    all_channel_stats = []\n",
    "    all_video_details = []\n",
    "    \n",
    "    # Track API call savings\n",
    "    total_new_videos = 0\n",
    "    total_cached_videos = 0\n",
    "    \n",
    "    # Process each channel\n",
    "    for index, row in channelid_data.iterrows():\n",
    "        CHANNEL_ID = row['channel_id']\n",
    "        logger.info(f\"Processing channel ID: {CHANNEL_ID}\")\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Processing channel ID: {CHANNEL_ID}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        channel_stats, video_details = main(CHANNEL_ID, youtube, incremental=incremental)\n",
    "        \n",
    "        all_channel_stats.append(channel_stats)\n",
    "        all_video_details.extend(video_details)\n",
    "        total_new_videos += len(video_details)\n",
    "    \n",
    "    # Create DataFrames\n",
    "    print(\"\\nCreating final DataFrames...\")\n",
    "    logger.info(\"Creating final DataFrames...\")\n",
    "    \n",
    "    channel_df = pd.DataFrame(all_channel_stats)\n",
    "    videos_df = pd.DataFrame(all_video_details)\n",
    "    \n",
    "    # ---- FIX DATA TYPES BEFORE SPARK ----\n",
    "    if len(videos_df) > 0:\n",
    "        numeric_cols = [\n",
    "            \"view_count\",\n",
    "            \"like_count\",\n",
    "            \"comment_count\",\n",
    "            \"duration\"\n",
    "        ]\n",
    "\n",
    "        for col in numeric_cols:\n",
    "            videos_df[col] = pd.to_numeric(videos_df[col], errors=\"coerce\").fillna(0).astype(\"int64\")\n",
    "\n",
    "    channel_numeric_cols = [\n",
    "        \"subscribers\",\n",
    "        \"total_views\",\n",
    "        \"video_count\"\n",
    "    ]\n",
    "\n",
    "    for col in channel_numeric_cols:\n",
    "        channel_df[col] = pd.to_numeric(channel_df[col], errors=\"coerce\").fillna(0).astype(\"int64\")\n",
    "\n",
    "    # Generate timestamp\n",
    "    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Save to CSV if requested (only if we have new data)\n",
    "    if save_format in ['csv', 'both']:\n",
    "        channel_filename = f'{DBFS_OUTPUT_PATH}/channel_stats_{timestamp}.csv'\n",
    "        videos_filename = f'{DBFS_OUTPUT_PATH}/video_details_{timestamp}.csv'\n",
    "        \n",
    "        channel_df.to_csv(channel_filename, index=False)\n",
    "        videos_df.to_csv(videos_filename, index=False)\n",
    "        \n",
    "        print(f\"\\nCSV files saved to:\")\n",
    "        print(f\"  - {channel_filename}\")\n",
    "        print(f\"  - {videos_filename}\")\n",
    "        logger.info(f\"CSV files saved to {DBFS_OUTPUT_PATH}\")\n",
    "        \n",
    "        # Upload to S3 if requested\n",
    "        if upload_to_s3 and S3_BUCKET:\n",
    "            print(f\"\\nUploading files to S3 bucket: {S3_BUCKET}...\")\n",
    "            logger.info(f\"Starting S3 upload to bucket: {S3_BUCKET}\")\n",
    "            \n",
    "            success_channel = upload_to_s3_from_databricks(channel_filename, S3_BUCKET)\n",
    "            success_videos = upload_to_s3_from_databricks(videos_filename, S3_BUCKET)\n",
    "            \n",
    "            if success_channel and success_videos:\n",
    "                print(\"\\n‚úì All files successfully uploaded to S3!\")\n",
    "                logger.info(\"All files successfully uploaded to S3\")\n",
    "            else:\n",
    "                print(\"\\n‚úó Some files failed to upload to S3. Check logs for details.\")\n",
    "                logger.warning(\"Some S3 uploads failed\")\n",
    "        elif upload_to_s3 and not S3_BUCKET:\n",
    "            print(\"\\n‚ö† S3 bucket not configured. Skipping S3 upload.\")\n",
    "            logger.warning(\"S3 bucket not configured, skipping S3 upload\")\n",
    "    \n",
    "    # Save to Delta tables if requested\n",
    "    if save_format in ['delta', 'both']:\n",
    "        save_to_delta_table(channel_df, f'youtube_channel_stats_{timestamp}', mode='overwrite')\n",
    "        if len(videos_df) > 0:\n",
    "            save_to_delta_table(videos_df, f'youtube_video_details_{timestamp}', mode='overwrite')\n",
    "        else:\n",
    "            print(\"‚ÑπÔ∏è No new videos to save to Delta table\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úì Pipeline execution completed successfully!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"üìä SUMMARY:\")\n",
    "    print(f\"   - Channels processed: {len(channel_df)}\")\n",
    "    print(f\"   - New videos fetched: {total_new_videos}\")\n",
    "    if incremental:\n",
    "        print(f\"   - Mode: INCREMENTAL (optimized)\")\n",
    "    else:\n",
    "        print(f\"   - Mode: FULL REFRESH\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    logger.info(\"Pipeline execution completed\")\n",
    "    \n",
    "    return channel_df, videos_df\n",
    "\n",
    "\n",
    "def clear_cache(channel_id=None):\n",
    "    \"\"\"\n",
    "    Clear the video cache for a specific channel or all channels.\n",
    "    \n",
    "    Use this when you want to do a full refresh of video data.\n",
    "    \n",
    "    Parameters:\n",
    "        channel_id (str): Specific channel ID to clear cache for.\n",
    "                         If None, clears all channel caches.\n",
    "    \"\"\"\n",
    "    if channel_id:\n",
    "        cache_file = get_cache_file_path(channel_id)\n",
    "        try:\n",
    "            import os\n",
    "            os.remove(cache_file)\n",
    "            print(f\"‚úì Cleared cache for channel: {channel_id}\")\n",
    "            logger.info(f\"Cleared cache for channel: {channel_id}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"‚ÑπÔ∏è No cache found for channel: {channel_id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error clearing cache: {e}\")\n",
    "    else:\n",
    "        # Clear all caches\n",
    "        try:\n",
    "            if IS_DATABRICKS:\n",
    "                dbfs_path = DBFS_CACHE_PATH.replace('/dbfs', 'dbfs:')\n",
    "                files = dbutils.fs.ls(dbfs_path)\n",
    "                for f in files:\n",
    "                    if f.name.endswith('_cache.json'):\n",
    "                        dbutils.fs.rm(f.path)\n",
    "                print(f\"‚úì Cleared all channel caches\")\n",
    "            else:\n",
    "                import os\n",
    "                import glob\n",
    "                for cache_file in glob.glob(f'{DBFS_CACHE_PATH}/*_cache.json'):\n",
    "                    os.remove(cache_file)\n",
    "                print(f\"‚úì Cleared all channel caches\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error clearing caches: {e}\")\n",
    "\n",
    "\n",
    "# ===== DATABRICKS NOTEBOOK EXECUTION =====\n",
    "# To run this in a Databricks notebook, execute:\n",
    "#\n",
    "# INCREMENTAL MODE (recommended for daily runs - saves API quota):\n",
    "# channel_df, videos_df = run_ingestion_pipeline(\n",
    "#     save_format='both',\n",
    "#     upload_to_s3=True,\n",
    "#     incremental=True  # Only fetch new videos\n",
    "# )\n",
    "#\n",
    "# FULL REFRESH MODE (use when you need all data):\n",
    "# channel_df, videos_df = run_ingestion_pipeline(\n",
    "#     save_format='both',\n",
    "#     upload_to_s3=True,\n",
    "#     incremental=False  # Fetch all videos\n",
    "# )\n",
    "#\n",
    "# TO CLEAR CACHE (before full refresh):\n",
    "# clear_cache()  # Clear all caches\n",
    "# clear_cache('UCxxxxxxx')  # Clear specific channel\n",
    "\n",
    "channel_df, videos_df = run_ingestion_pipeline(\n",
    "    save_format='both',  # Save as both CSV and Delta tables\n",
    "    upload_to_s3=True,\n",
    "    incremental=True  # Set to False for full refresh\n",
    ")\n",
    "# display(channel_df)\n",
    "# display(videos_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": {
    "autoRunOnWidgetChange": "no-auto-run"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ingestions_databricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
